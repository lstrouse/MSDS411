---
title: "Auto Insurance R Report"
author: "Logan Strouse"
date: "5/14/2019"
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r coding, eval = TRUE, echo = FALSE, message = FALSE, warning = FALSE}
# Loading libraries that will be used
library(readr)
library(dplyr)
library(plyr)
library(zoo)
library(psych)
library(ROCR)
library(corrplot)
library(car)
library(InformationValue)
library(pbkrtest)
library(car)
library(leaps)
library(MASS)
library(corrplot)
library(glm2)
library(aod)
library(mice)
library(VIM)
library(MASS)
library(leaps)
library(rpart)
library(knitr)
library(ROCR)

setwd("~/Desktop/NW MSDS/MSDS411/Unit 2/Auto Insurance/")
train <- read.csv("logit_insurance.csv")
test <- read.csv("logit_insurance_test.csv")
```

```{r adjustments, eval = TRUE, echo = FALSE, message = FALSE, warning = FALSE}
#Assigning Factor Levels, Numerical & Categorical Checks

#Train Data Adjustments
train$INDEX <- as.factor(train$INDEX)
train$TARGET_FLAG <- as.factor(train$TARGET_FLAG)
train$SEX <- as.factor(train$SEX)
train$EDUCATION <- as.factor(train$EDUCATION)
train$PARENT1 <- as.factor(train$PARENT1)
train$INCOME <- suppressWarnings(as.numeric(gsub("[^0-9.]", "", train$INCOME)))
train$HOME_VAL <- suppressWarnings(as.numeric(gsub("[^0-9.]", "", train$HOME_VAL)))
train$MSTATUS <- as.factor(train$MSTATUS)
train$REVOKED <- as.factor(train$REVOKED)
train$RED_CAR <- as.factor(ifelse(train$RED_CAR=="yes", 1, 0))
train$URBANICITY <- ifelse(train$URBANICITY == "Highly Urban/ Urban", "Urban", "Rural")
train$URBANICITY <- as.factor(train$URBANICITY)
train$JOB <- as.factor(train$JOB)
train$CAR_USE <- as.factor(train$CAR_USE)
train$CAR_TYPE <- as.factor(train$CAR_TYPE)
train$DO_KIDS_DRIVE <- as.factor(ifelse(train$KIDSDRIV > 0, 1, 0 ))
train$OLDCLAIM <- suppressWarnings(as.numeric(gsub("[^0-9.]", "", train$HOME_VAL)))
train$BLUEBOOK <- suppressWarnings(as.numeric(gsub("[^0-9.]", "", train$BLUEBOOK)))
#summary(train)
#write.csv(train_predicts, file = "trainref.csv")

#Test Data Adjustments 
test$INDEX <- as.factor(test$INDEX)
test$TARGET_FLAG <- as.factor(test$TARGET_FLAG)
test$SEX <- as.factor(test$SEX)
test$EDUCATION <- as.factor(test$EDUCATION)
test$PARENT1 <- as.factor(test$PARENT1)
test$INCOME <- suppressWarnings(as.numeric(gsub("[^0-9.]", "", test$INCOME)))
test$HOME_VAL <- suppressWarnings(as.numeric(gsub("[^0-9.]", "", test$HOME_VAL)))
test$MSTATUS <- as.factor(test$MSTATUS)
test$REVOKED <- as.factor(test$REVOKED)
test$RED_CAR <- as.factor(ifelse(test$RED_CAR=="yes", 1, 0))
test$URBANICITY <- ifelse(test$URBANICITY == "Highly Urban/ Urban", "Urban", "Rural")
test$URBANICITY <- as.factor(test$URBANICITY)
test$JOB <- as.factor(test$JOB)
test$CAR_USE <- as.factor(test$CAR_USE)
test$CAR_TYPE <- as.factor(test$CAR_TYPE)
test$DO_KIDS_DRIVE <- as.factor(ifelse(test$KIDSDRIV > 0, 1, 0 ))
test$OLDCLAIM <- suppressWarnings(as.numeric(gsub("[^0-9.]", "", test$HOME_VAL)))
test$BLUEBOOK <- suppressWarnings(as.numeric(gsub("[^0-9.]", "", test$BLUEBOOK)))
#summary(test)
```

```{r dataexplore, message=FALSE, warning=FALSE, include=FALSE}
#EDA
#cor.ci(train, method ="spearman" )
#aggr_plot <- aggr(train, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(train), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))
#aggr_plot
#MICE Analysis
#md.pairs(train)
#pbox(train,pos=1,int=TRUE,cex=0.7)
#imp <- mice(train)
#cleaning and identifying missing values train (N/A)
#attaching and reattaching index due to issues with index
sapply(train, function(x) sum(is.na(x)))
```


```{r dataexploremice, message=FALSE, warning=FALSE, include=FALSE}
INDEX_TRAIN <- train[["INDEX"]]
train <- complete(mice(train[,2:27]))
train$INDEX <- INDEX_TRAIN
train <- train[,c(27,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26)]
```

```{r dataexploretrain, message=FALSE, warning=FALSE, include=FALSE}
sapply(train, function(x) sum(is.na(x)))
#need to fix oldclaim
train$OLDCLAIM <- ifelse(train$CAR_AGE < 5 & !is.na(train$CAR_AGE),0,train$OLDCLAIM)
train$OLDCLAIM <- na.aggregate(train$OLDCLAIM, train$CAR_AGE, mean, na.rm = TRUE )
sapply(train, function(x) sum(is.na(x)))
```

```{r dataexploretest, message=FALSE, warning=FALSE, include=FALSE}
#cleaning and identifying missing values test (N/A)
#attaching and reattaching index due to issues with index
sapply(test, function(x) sum(is.na(x)))
INDEX_TEST <- test[["INDEX"]]
test <- complete(mice(test[,2:27]))
test$INDEX <- INDEX_TEST
test <- test[,c(27,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26)]
sapply(test, function(x) sum(is.na(x)))
#need to fix oldclaim
test$OLDCLAIM <- ifelse(test$CAR_AGE < 5 & !is.na(test$CAR_AGE),0,test$OLDCLAIM)
test$OLDCLAIM <- na.aggregate(test$OLDCLAIM, test$CAR_AGE, mean, na.rm = TRUE )
sapply(test, function(x) sum(is.na(x)))
```

```{r carcomps, include=FALSE}
x <- train$CAR_AGE 
h<-hist(x, breaks=12, col="red", xlab="Car Age", 
        main="Normal Curve and Histogram of Car Age",labels = TRUE) 
xfit<-seq(min(x),max(x),length=40) 
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x)) 
yfit <- yfit*diff(h$mids[1:2])*length(x) 
lines(xfit, yfit, col="blue", lwd=2)
```

```{r countsbothsideside, include=FALSE}
t1 <- count(train, 'CAR_AGE')
t2 <- count(train, 'CAR_TYPE')
knitr::kable(list(t1, t2))
```

```{r incomeexplore, include=FALSE}
hist(train$INCOME,col= 'red', breaks= 10)
boxplot(train$INCOME)
```

```{r morecode, include=FALSE}
#binned income based on IQR ranges Train
train$INCOME_bin[is.na(train$INCOME)] <- "NA"
train$INCOME_bin[train$INCOME >= 0] <- "Zero"
train$INCOME_bin[train$INCOME >= 1 & train$INCOME < 27907] <- "Low"
train$INCOME_bin[train$INCOME >= 27907 & train$INCOME < 53841] <- "Medium"
train$INCOME_bin[train$INCOME >= 53841 & train$INCOME < 85731] <- "High"
train$INCOME_bin[train$INCOME >= 85731] <- "Affluent"
train$INCOME_bin <- factor(train$INCOME_bin)
train$INCOME_bin <- factor(train$INCOME_bin, levels=c("NA","Zero","Low","Medium","High","Affluent"))

#binned income based on IQR ranges Test
test$INCOME_bin[is.na(test$INCOME)] <- "NA"
test$INCOME_bin[test$INCOME >= 0] <- "Zero"
test$INCOME_bin[test$INCOME >= 1 & test$INCOME < 27907] <- "Low"
test$INCOME_bin[test$INCOME >= 27907 & test$INCOME < 53841] <- "Medium"
test$INCOME_bin[test$INCOME >= 53841 & test$INCOME < 85731] <- "High"
test$INCOME_bin[test$INCOME >= 85731] <- "Affluent"
test$INCOME_bin <- factor(test$INCOME_bin)
test$INCOME_bin <- factor(test$INCOME_bin, levels=c("NA","Zero","Low","Medium","High","Affluent"))


#Adding additional variables to the dataset to data frames to use
train$HOME_OWNER <- ifelse(train$HOME_VAL == 0, 0, 1)
test$HOME_OWNER <- ifelse(test$HOME_VAL == 0, 0, 1)
train$NEW_USED_CAR <- ifelse(train$CAR_AGE <= 2, 0, 1)
test$NEW_USED_CAR <- ifelse(test$CAR_AGE <= 2, 0, 1)
```

```{r correlations, include=FALSE}
#correlations of numeric data
numeric_data <- subset(train, select = c(TARGET_AMT, AGE, HOMEKIDS,KIDSDRIV,OLDCLAIM, YOJ, INCOME, HOME_VAL,HOME_OWNER,NEW_USED_CAR, TRAVTIME, BLUEBOOK, TIF,
                                   CLM_FREQ, MVR_PTS, CAR_AGE), na.rm = TRUE)
corrplot(cor(numeric_data), method = "square")

train_predicts<-data.frame(train)
```

```{r models, include=FALSE}
#model development
complete_model <- glm(TARGET_FLAG ~  AGE + BLUEBOOK + TRAVTIME + KIDSDRIV + SEX + URBANICITY + HOMEKIDS + INCOME + OLDCLAIM + DO_KIDS_DRIVE + HOME_OWNER + NEW_USED_CAR +
                        CLM_FREQ + REVOKED + MVR_PTS + CAR_AGE + TIF + EDUCATION + MSTATUS + PARENT1 + RED_CAR + 
                        CAR_USE + CAR_TYPE + YOJ + JOB + INCOME_bin + HOME_VAL, 
              data = train, family = binomial())
summary(complete_model)
train_predicts$Model1Prediction <- predict(complete_model, type = "response")
vif(complete_model)

Model2 <- glm(TARGET_FLAG ~  AGE + BLUEBOOK + TRAVTIME + KIDSDRIV + SEX + URBANICITY + HOMEKIDS + INCOME + OLDCLAIM + DO_KIDS_DRIVE + HOME_OWNER + NEW_USED_CAR +
                CLM_FREQ + REVOKED + MVR_PTS + TIF + EDUCATION + MSTATUS + PARENT1 + RED_CAR + 
                CAR_USE + CAR_TYPE + YOJ + JOB + INCOME_bin,
              data = train, family = binomial())
summary(Model2)
train_predicts$Model2Prediction <- predict(Model2, type = "response")

#backwards/step selection
backwards_model = step(complete_model)
formula(backwards_model)
summary(backwards_model)
train_predicts$Model3Prediction <- predict(backwards_model, type = "response")


#regsubsets
regfit.full = regsubsets(TARGET_FLAG ~ AGE + BLUEBOOK + TRAVTIME + KIDSDRIV + SEX + URBANICITY + HOMEKIDS + INCOME + OLDCLAIM + DO_KIDS_DRIVE + HOME_OWNER + NEW_USED_CAR +
                           CLM_FREQ + REVOKED + MVR_PTS + CAR_AGE + TIF + EDUCATION + MSTATUS + PARENT1 + RED_CAR + 
                           CAR_USE + CAR_TYPE + YOJ + JOB + INCOME_bin + HOME_VAL,nvmax = 10, nbest = 1,  data = train)
summary(regfit.full)
Model4 <- glm(TARGET_FLAG ~  URBANICITY + INCOME + JOB + MSTATUS + TIF + DO_KIDS_DRIVE + TRAVTIME + BLUEBOOK + REVOKED + MVR_PTS + CAR_USE,
              data = train, family = binomial())
summary(Model4)
train_predicts$Model4Prediction <- predict(Model4, type = "response")

#rfit model
rfit = rpart(TARGET_FLAG ~ AGE + BLUEBOOK + TRAVTIME + KIDSDRIV + SEX + URBANICITY + HOMEKIDS + INCOME + OLDCLAIM + DO_KIDS_DRIVE + HOME_OWNER + NEW_USED_CAR +
               CLM_FREQ + REVOKED + MVR_PTS + CAR_AGE + TIF + EDUCATION + MSTATUS + PARENT1 + RED_CAR + 
               CAR_USE + CAR_TYPE + YOJ + JOB + INCOME_bin + HOME_VAL, data = train , method = "class", cp = 0.0001)
summary(rfit)
train_predicts$Model5Prediction <- predict(rfit)

#mse
mse <- function(sm) 
  mean(sm$residuals^2)
```


```{r statistics, include=FALSE}
cat("Complete Model AIC:",AIC(complete_model))
cat("Model 2 AIC:",AIC(Model2))
cat("Backwards Model AIC:",AIC(backwards_model))
cat("Model 4 AIC:",AIC(Model4))
#AIC(rfit)
cat("Complete Model BIC:",BIC(complete_model))
cat("Model 2 BIC:",BIC(Model2))
cat("Backwards Model BIC:",BIC(backwards_model))
cat("Model 4 BIC:",BIC(Model4))
#BIC(rfit)
print(-2*logLik(complete_model, REML = TRUE))
print(-2*logLik(Model2, REML = TRUE))
print(-2*logLik(backwards_model, REML = TRUE))
print(-2*logLik(Model4, REML = TRUE))
#print(-2*logLik(rfit, REML = TRUE))
ks_stat(actuals=train$TARGET_FLAG, predictedScores=train_predicts$Model1Prediction)
ks_stat(actuals=train$TARGET_FLAG, predictedScores=train_predicts$Model2Prediction)
ks_stat(actuals=train$TARGET_FLAG, predictedScores=train_predicts$Model3Prediction)
ks_stat(actuals=train$TARGET_FLAG, predictedScores=train_predicts$Model4Prediction)
#ks_stat(actuals=train$TARGET_FLAG, predictedScores=train_predicts$Model5Prediction)
```

```{r stats2, include=FALSE}
#pred <- prediction(train_predicts$Model1Prediction, train$TARGET_FLAG)
#perf <- performance(pred,"tpr","fpr")
#performance(pred,'auc')
#model1roc <- plot(perf,colorize=TRUE)

#pred2 <- prediction(train_predicts$Model2Prediction, train$TARGET_FLAG)
#perf <- performance(pred2,"tpr","fpr")
#performance(pred2,'auc')
#model2roc <- plot(perf,colorize=TRUE)

#pred3 <- prediction(train_predicts$Model3Prediction, train$TARGET_FLAG)
#perf <- performance(pred3,"tpr","fpr")
#performance(pred3,'auc')
#model3roc <- plot(perf,colorize=TRUE)

#pred4 <- prediction(train_predicts$Model4Prediction, train$TARGET_FLAG)
#perf <- performance(pred4,"tpr","fpr")
#performance(pred4,'auc')
#model4roc <- plot(perf,colorize=TRUE)
```


```{r rest of code, include=FALSE}
#choosing backwards model
coef(backwards_model)
summary(backwards_model)
```


```{r rest of code end, include=FALSE}
#Target_AMT Model
subsets_AMT <- regsubsets(TARGET_AMT ~ AGE + BLUEBOOK + TRAVTIME + KIDSDRIV + SEX + URBANICITY + HOMEKIDS + INCOME + OLDCLAIM + DO_KIDS_DRIVE + HOME_OWNER + NEW_USED_CAR +
                            CLM_FREQ + REVOKED + MVR_PTS + CAR_AGE + TIF + EDUCATION + MSTATUS + PARENT1 + RED_CAR + 
                            CAR_USE + CAR_TYPE + YOJ + JOB + INCOME_bin + HOME_VAL,nvmax = 10, nbest = 1,  data = train)
subsets_AMT
summary(subsets_AMT)
plot(subsets_AMT, scale="adjr2")
subset_amt_model <- lm(TARGET_AMT ~ TRAVTIME + URBANICITY + INCOME + DO_KIDS_DRIVE + MVR_PTS + CAR_AGE + TIF + MSTATUS + PARENT1 + CAR_USE + JOB, data = train)
summary(subset_amt_model)
train_predicts$subsets_amt <- predict(subset_amt_model)


#Predicting
test$P_TARGET_FLAG <- predict(backwards_model, newdata = test, type = "response")
test$P_TARGET_AMT<- predict(subset_amt_model, newdata = test, type = "response")
test$P_TARGET_AMT[(test$P_TARGET_AMT <= 0)] = 0

#Scored File
scores <- test[c("INDEX","P_TARGET_FLAG", "P_TARGET_AMT")]
write.csv(scores, file = "LOGAN_STROUSE_Scored_INSURANCE.csv")
```

## Bingo Bonus
  For this assignment I would like the opportunity to recieve 5-10 points for doing the writeup and execution within an RMD document. This was a good learning experience for myself due to my previous courses being in Python and not having much experience with Rstudio. This document seems to have a better functionality than Jupyter Notebooks with Python. I also used rpart decision trees to build a model, unfortunely that model wasn't chosen but the code is in the raw R file. Thanks in Advance!

## Introduction
***
   This assignment was a look at investigating different logistic regression models to predict a binary varaible value. There was also a continous variable that predicted the amount of a possible claim that also needed to be modeled. In order to successfully build a prediction model, I had to clear up missing values and also create a few new variables that could possibly aid to help increase the accuracy of the model. Multiple techniques were used to correct the missing values in both the training and testing data sets. These included manual imputation through limits as well as using auto-imputation through packages, like mice. Various statistics were also used to pick the winning model, amoung those chosen include AIC, BIC and the AUC. 

## Data Exploration
***
  Exploring the data was key to determining the relationships between the variables. It is also helpful to see where there are possible outliars and missing data. There were a total of 8,161 observations and 25 variables (not counting index) in the training data set. The test data set had 2,141 observations. Having 80% of the data available in the traiing dataset helped to develope a stronger model and the sample size was sufficient. Upon some basic charting, I was able to use the correlation plot below to see which variables had strong relationships amongst each other. I used this for part of my approach to builing the models and was mindful of some of the possible multicollinearity. Kids driving and Home kids were examples of something to be mindful of for example.  

```{r corrdisplay, echo=FALSE}
#correlations of numeric data
numeric_data <- subset(train, select = c(TARGET_AMT, AGE, HOMEKIDS,KIDSDRIV,OLDCLAIM, YOJ, INCOME, HOME_VAL,HOME_OWNER,NEW_USED_CAR, TRAVTIME, BLUEBOOK, TIF,
                                   CLM_FREQ, MVR_PTS, CAR_AGE), na.rm = TRUE)
corrplot(cor(numeric_data), method = "square")
```

## Data Preparation
***
  The data preparation step of the project was one of the most time-intensive steps of the project. During this part of the project the data was adjusted to relect the correct type. This included telling R to look at the variables in the correct form of integer,numeric or factor. It was at this point that I Binned the income to 6 levels as well. I used the IQR range as the main reasons for breaking down the pay groups into semi-equal groups. I also created two other new variables for assessment. They were binary variables of Home_Owner and NEW_USED_CAR. I created the NEW_USED_CAR variable based upon the distribution of the car age data. Below is a plot I created with the normal curve to exemplify this. Based upon the heavily weighted 1 year age, I signified that as a prime candidate to be a new car. The thought behind this as well, was to prove that new cars might or might not be prone to more claims.

```{r carscounting, echo=FALSE}
x <- train$CAR_AGE 
h<-hist(x, breaks=12, col="red", xlab="Car Age", 
        main="Normal Curve and Histogram of Car Age",labels = TRUE) 
xfit<-seq(min(x),max(x),length=40) 
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x)) 
yfit <- yfit*diff(h$mids[1:2])*length(x) 
lines(xfit, yfit, col="blue", lwd=2)
t1 <- count(train, 'CAR_AGE')
t2 <- count(train, 'CAR_TYPE')
knitr::kable(list(t1, t2))
```
  
  It was also during this stage that I used the mice package to fill missing values using predictive mean matching. The mice package did not work well on the Old Claim field, so I hand inputed that with the mean of the column for the missing values. Later in the project I also imputed negative values to zero on claim amounts. I did not see the idea of refunds as being applicable for this dataset.  The sapply function was applied to check and confirm the values for both test and train were taken care of. An example of that is below. 
  
```{r Sapply, echo=FALSE}
sapply(train, function(x) sum(is.na(x)))
```

## Build Models
  Throughout the course of the project I built multiple models. My three best are below. They included a standard full logistic regression model, backwards step selection model and a regsubsets model that selected variables based on variable numbers. I also tried some other minor variations to each model and was not able to improve thier AIC or AUC scores. Below are the three models that were eligible to be selected towards being chosen as a champion model. Overall the models all had similar AIC/BIC scores and AUC scores. The coeffients in the logistic regression model are harder to explain and with them being small numbers it was difficult to decyper the differences between the models based simply on that. They all seemed to show the same pattern, and that is why the scoring was important and mentioned in the next section. 

### Complete Model
```{r complete_MODEL, echo=FALSE}
summary(complete_model)
cat("Complete Model AIC:",AIC(complete_model))
cat("Complete Model BIC:",BIC(complete_model))
```
### Backwards Stepwise Model
```{r model_2, echo=FALSE}
summary(backwards_model)
cat("Backwards Model AIC:",AIC(backwards_model))
cat("Backwards Model BIC:",BIC(backwards_model))
```
### Reg Subsets Model
```{r model_3, echo=FALSE}
summary(Model4)
cat("Reg Subsets Model AIC:",AIC(Model4))
cat("Reg Subsets Model BIC:",BIC(Model4))
```


## Select Model
 The winning model that had the best AIC with the most area under the curve was the backwards stepwise model. The AUC was .8150246 and the AIC was 7347.265. It is shown below along with the other two models that were shown prior with their AUC charts KS statistics. I decided based on my readings and prior work that AIC and the AUC statistic were the most important, since MSE is not great for logistic models. For reference in the output below, the y.values are the AUC scores and the ks statistic is the last number mentioned with each model that begins with a .47.

### Backwards Stepwise Model

```{r champ, echo=FALSE}
pred3 <- prediction(train_predicts$Model3Prediction, train$TARGET_FLAG)
perf <- performance(pred3,"tpr","fpr")
performance(pred3,'auc')
model3roc <- plot(perf,colorize=TRUE)
ks_stat(actuals=train$TARGET_FLAG, predictedScores=train_predicts$Model3Prediction)
```

### Complete Model

```{r complete non, echo=FALSE}
pred <- prediction(train_predicts$Model1Prediction, train$TARGET_FLAG)
perf <- performance(pred,"tpr","fpr")
performance(pred,'auc')
model1roc <- plot(perf,colorize=TRUE)
ks_stat(actuals=train$TARGET_FLAG, predictedScores=train_predicts$Model1Prediction)
```


### Reg Subsets Model

```{r REG NON, echo=FALSE}
pred4 <- prediction(train_predicts$Model4Prediction, train$TARGET_FLAG)
perf <- performance(pred4,"tpr","fpr")
performance(pred4,'auc')
model4roc <- plot(perf,colorize=TRUE)
ks_stat(actuals=train$TARGET_FLAG, predictedScores=train_predicts$Model4Prediction)
```


## Stand Alone Scoring Program

P_Target_AMT Scoring Program (includes correction for negative values)
test$P_TARGET_AMT<- predict(subset_amt_model, newdata = test, type = "response")
test$P_TARGET_AMT[(test$P_TARGET_AMT <= 0)] = 0

P_Target_Flag Scoring Program
test$P_TARGET_FLAG <- predict(backwards_model, newdata = test, type = "response")

The formulas for both of these are derived off of the Model Coefficients in the summaries for both model's above.

## Scored Data Set
This was submitted separately.

## Conclusion
  Overall, this assignment was a great look at the aspects that can be used to predict risk and potential outcome of events. I believe that a similar model could be deployed in many other industries like the credit industry or even potential employement opportunites. Unfortunetly, there are some ethical issues on the later option. There is other data that might have helped with this data set. If we could get our hands on some IOT data that contains speed at time of crash, brake use and other telemetrics we might be able to refine the accuracy of these models even further. 

